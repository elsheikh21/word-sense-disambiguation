{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 477.79999999999995,
      "position": {
        "height": "40px",
        "left": "1266px",
        "right": "20px",
        "top": "120px",
        "width": "250px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "varInspector_section_display": "none",
      "window_display": true
    },
    "colab": {
      "name": "WSD.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OEuC1-2grw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'http://lcl.uniroma1.it/wsdeval/data/WSD_Unified_Evaluation_Datasets.zip'\n",
        "!wget 'http://lcl.uniroma1.it/wsdeval/data/WSD_Evaluation_Framework.zip'\n",
        "\n",
        "!mkdir resources\n",
        "!rm -rf sample_data\n",
        "\n",
        "!unzip  'WSD_Evaluation_Framework.zip'\n",
        "!unzip  'WSD_Unified_Evaluation_Datasets.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T17:28:23.533480Z",
          "start_time": "2019-08-16T17:28:23.523805Z"
        },
        "id": "5qmApSORf5nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from lxml.etree import iterparse\n",
        "from nltk.corpus import wordnet as wn\n",
        "from tensorflow.keras.preprocessing.text import (Tokenizer,\n",
        "                                                 text_to_word_sequence)\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T17:27:48.573374Z",
          "start_time": "2019-08-16T17:27:48.569383Z"
        },
        "code_folding": [
          0,
          5
        ],
        "id": "oxlzc4Abf5nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_json(save_to, save_what):\n",
        "    with open(save_to, 'w+') as json_file:\n",
        "        json.dump(save_what, json_file)\n",
        "\n",
        "\n",
        "def save_pickle(save_to, save_what):\n",
        "    with open(save_to, mode='wb') as f:\n",
        "        pickle.dump(save_what, f)\n",
        "\n",
        "\n",
        "def load_pickle(load_from):\n",
        "    with open(load_from, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NnZulHynEtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def configure_tf():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    config = tf.ConfigProto()\n",
        "    # dynamically grow the memory used on the GPU\n",
        "    config.gpu_options.allow_growth = True\n",
        "    # to log device placement (on which device the operation ran)\n",
        "    config.log_device_placement = True\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
        "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
        "    sess = tf.Session(config=config)\n",
        "    # set this TensorFlow session as the default session for Keras\n",
        "    set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T17:27:49.057035Z",
          "start_time": "2019-08-16T17:27:49.046956Z"
        },
        "code_folding": [
          0
        ],
        "id": "_BXjHIRef5nS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_logger():\n",
        "    \"\"\"\n",
        "    Customize the logger, and fixes seed\n",
        "    \"\"\"\n",
        "    np.random.seed(0)\n",
        "    logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
        "                        datefmt='%H:%M:%S', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:11:53.264994Z",
          "start_time": "2019-08-16T18:11:53.257015Z"
        },
        "code_folding": [
          25
        ],
        "id": "ivM5dmM6f5nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(file_name, save_to=None):\n",
        "    '''\n",
        "    Builds and saves dictionary from text file\n",
        "    This dictionary contains all the senses of all words\n",
        "    '''\n",
        "    if save_to is not None and os.path.exists(save_to) and os.path.getsize(save_to) > 0:\n",
        "        file_dict = load_pickle(save_to)\n",
        "        logging.info(\"Dictionary is loaded\")\n",
        "    else:\n",
        "        file_dict = dict()\n",
        "        with open(file_name, mode='r') as file:\n",
        "            lines = file.read().splitlines()\n",
        "            for line in tqdm(lines, desc='Building dictionary'):\n",
        "                synset_id, synset = line.split()[0], line.split()[1]\n",
        "                file_dict[synset_id] = synset\n",
        "        logging.info(\"Dictionary is built\")\n",
        "        if save_to is not None:\n",
        "            save_pickle(save_to, file_dict)\n",
        "        logging.info(\"Dictionary is saved\")\n",
        "    return file_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:32:43.419559Z",
          "start_time": "2019-08-16T18:32:43.398608Z"
        },
        "code_folding": [
          0,
          54
        ],
        "id": "dxKGLIhGf5na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_dataset(file_name, gold_dict, save_to_paths=None):\n",
        "    '''\n",
        "    Starts with reading xml file, only sentence tags, iterates over children\n",
        "    of sentences' tags, if it word-format or instance we add lemma to form\n",
        "    our sentence, if instance we add it in form of lemma_synsetID.\n",
        "\n",
        "    :param file_name: string points to path of xml file\n",
        "    :param gold_dict: dict contains all senses as per xml file\n",
        "    :return sentences: list of strings contains all data unlabeled\n",
        "    :return sentences_labeled: list of strings contains all data labeled\n",
        "    '''\n",
        "    if save_to_paths is not None and os.path.exists(save_to_paths[0]) and os.path.getsize(save_to_paths[0]) > 0 and os.path.exists(save_to_paths[1]) and os.path.getsize(save_to_paths[1]) > 0:\n",
        "        sentences_list = load_pickle(save_to_paths[0])\n",
        "        lbld_sentences_list = load_pickle(save_to_paths[1])\n",
        "        logging.info(\"Parsed Dataset is loaded\")\n",
        "    else:\n",
        "        # read file contents in terms of sentences\n",
        "        context = iterparse(file_name, tag=\"sentence\")\n",
        "        sentences_list, lbld_sentences_list = [], []\n",
        "        # iterating over the sentences\n",
        "        for _, elements in tqdm(context, desc=\"Parsing corpus\"):\n",
        "            sentence, sentence_labeled = [], []\n",
        "            for elem in list(elements.iter()):\n",
        "                if elem is not None:\n",
        "                    # if tag is word-format (wf) or instance\n",
        "                    if (elem.tag == 'wf' or elem.tag == 'instance') and elem.text is not None:\n",
        "                        elem_lemma = elem.attrib['lemma']\n",
        "                        sentence.append(elem_lemma)\n",
        "                        sentence_labeled.append(elem_lemma)\n",
        "                    if elem.tag == 'instance' and elem.text is not None:\n",
        "                        elem_id = elem.attrib['id']\n",
        "                        elem_lemma = elem.attrib['lemma']\n",
        "                        sense_key = str(gold_dict.get(elem_id))\n",
        "                        if sense_key is not None:\n",
        "                            synset = wn.lemma_from_key(sense_key).synset()\n",
        "                            synset_id = f\"wn:{str(synset.offset()).zfill(8)}{synset.pos()}\"\n",
        "                            sentence_labeled[-1] = f'{synset_id}'\n",
        "            # if the sentence is not empty\n",
        "            if len(sentence) and len(sentence_labeled):\n",
        "                sentences_list.append(sentence)\n",
        "                lbld_sentences_list.append(sentence_labeled)\n",
        "            elements.clear()\n",
        "        logging.info(\"Parsed the dataset\")\n",
        "\n",
        "        if save_to_paths is not None:\n",
        "            save_x_to, save_y_to = save_to_paths[0], save_to_paths[1]\n",
        "            save_pickle(save_x_to, sentences_list)\n",
        "            save_pickle(save_y_to, lbld_sentences_list)\n",
        "            logging.info(\"Saved the dataset\")\n",
        "\n",
        "    return sentences_list, lbld_sentences_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9B99Zkgj5oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_dataset(data_x, data_y, save_tokenizer=None, save_data=None):\n",
        "    if (save_data[0] is not None\n",
        "            and os.path.exists(save_data[0])\n",
        "            and os.path.getsize(save_data[0]) > 0):\n",
        "        data_x = load_pickle(save_data[0])\n",
        "        logging.info(\"data_x is loaded\")\n",
        "    if (save_data[1] is not None\n",
        "            and os.path.exists(save_data[1])\n",
        "            and os.path.getsize(save_data[1]) > 0):\n",
        "        data_y = load_pickle(save_data[1])\n",
        "        logging.info(\"data_y is loaded\")\n",
        "\n",
        "    if (save_tokenizer is not None\n",
        "            and os.path.exists(save_tokenizer)\n",
        "            and os.path.getsize(save_tokenizer) > 0):\n",
        "        tokenizer = load_pickle(save_tokenizer)\n",
        "        logging.info(\"Tokenizer is loaded\")\n",
        "        tokenizer.fit_on_texts(data_x)\n",
        "        tokenizer.fit_on_texts(data_y)\n",
        "    else:\n",
        "        filters = '!\"#$%&()*+,-./;<=>?@[\\\\]^_`{|}~\\'\\t'\n",
        "        tokenizer = Tokenizer(filters=filters, oov_token='<OOV>', lower=True)\n",
        "        tokenizer.fit_on_texts(data_x)\n",
        "        tokenizer.fit_on_texts(data_y)\n",
        "        if save_tokenizer is not None:\n",
        "            save_pickle(save_tokenizer, tokenizer)\n",
        "            logging.info(\"Tokenizer Saved\")\n",
        "\n",
        "    if save_data is not None:\n",
        "        save_pickle(save_data[0], data_x)\n",
        "        save_pickle(save_data[1], data_y)\n",
        "        logging.info(\"Processed Data is Saved\")\n",
        "\n",
        "    data_x_ = tokenizer.texts_to_sequences(data_x)\n",
        "    data_y_ = tokenizer.texts_to_sequences(data_y)\n",
        "\n",
        "    return data_x_, data_y_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:33:37.386083Z",
          "start_time": "2019-08-16T18:33:37.376078Z"
        },
        "code_folding": [],
        "id": "vekbtgOpf5nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "    # Building the gold dictionary\n",
        "    cwd = os.getcwd()\n",
        "    data_path = os.path.join(cwd, 'data')\n",
        "    resources_path = os.path.join(cwd, 'resources')\n",
        "\n",
        "    # Building the gold dictionary for training set\n",
        "    file_path = os.path.join(\n",
        "        cwd, 'WSD_Evaluation_Framework', 'Training_Corpora',\n",
        "        'SemCor', 'semcor.gold.key.txt')\n",
        "    save_to = os.path.join(resources_path, 'gold_dict.pkl')\n",
        "    gold_dict = build_dict(file_path, save_to)\n",
        "\n",
        "    # parsing the dataset\n",
        "    path = os.path.join(cwd, 'WSD_Evaluation_Framework', 'Training_Corpora', 'SemCor', 'semcor.data.xml')\n",
        "    save_data = [os.path.join(resources_path, 'train_x.pkl'),\n",
        "                 os.path.join(resources_path, 'train_y.pkl')]\n",
        "    (data_x, data_y) = parse_dataset(path, gold_dict, save_to_paths=save_data)\n",
        "\n",
        "    save_tokenizer = os.path.join(resources_path, 'tokenizer.pkl')\n",
        "    train_x, train_y = process_dataset(data_x, data_y, save_tokenizer=save_tokenizer, save_data=save_data)\n",
        "        \n",
        "    # Building the gold dictionary for dev set\n",
        "    eval_path = os.path.join('WSD_Unified_Evaluation_Datasets', 'ALL', 'ALL.data.xml')\n",
        "\n",
        "    eval_gold = os.path.join('WSD_Unified_Evaluation_Datasets', 'ALL', 'ALL.gold.key.txt')\n",
        "\n",
        "    # Parsing the gold dict\n",
        "    save_eval_to = os.path.join(resources_path, 'eval_dict.pkl')\n",
        "    eval_dict = build_dict(eval_gold, save_eval_to)\n",
        "\n",
        "    # Parsing the dev dataset\n",
        "    save_data = [os.path.join(resources_path, 'dev_x.pkl'),\n",
        "                 os.path.join(resources_path, 'dev_y.pkl')]\n",
        "    (data_x, data_y) = parse_dataset(eval_path, eval_dict,\n",
        "                                     save_to_paths=save_data)\n",
        "    save_tokenizer = os.path.join(resources_path, 'tokenizer.pkl')\n",
        "    dev_x, dev_y = process_dataset(data_x, data_y,\n",
        "                                   save_tokenizer=save_tokenizer,\n",
        "                                   save_data=save_data)\n",
        "    \n",
        "    tokenizer = load_pickle(save_tokenizer)\n",
        "    word_tokens = [\n",
        "        word for word in tokenizer.word_index if not word.startswith('wn:')]\n",
        "    sense_tokens = [\n",
        "        word for word in tokenizer.word_index if word.startswith('wn:')]\n",
        "\n",
        "    vocabulary_size = len(word_tokens) + 1\n",
        "    output_size = vocabulary_size + len(sense_tokens) + 1\n",
        "\n",
        "    dataset = {\n",
        "        'train_x': train_x,\n",
        "        'train_y': train_y,\n",
        "        'dev_x': dev_x,\n",
        "        'dev_y': dev_y,\n",
        "        'tokenizer': load_pickle(save_tokenizer),\n",
        "        'vocabulary_size': vocabulary_size,\n",
        "        'output_size': output_size\n",
        "    }\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk91vgBuTN1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initialize_logger()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j78M1Nr57xLu",
        "colab_type": "code",
        "outputId": "47c247ea-f5fa-4b62-a3ad-ad868407bc0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MScmAXWA7Q0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "0ad041f4-95e1-4167-d069-be4d9087cd69"
      },
      "source": [
        "dataset = load_dataset()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 17:05:39: Dictionary is loaded\n",
            "INFO - 17:05:39: Parsed Dataset is loaded\n",
            "INFO - 17:05:39: data_x is loaded\n",
            "INFO - 17:05:40: data_y is loaded\n",
            "INFO - 17:05:40: Tokenizer is loaded\n",
            "INFO - 17:05:41: Processed Data is Saved\n",
            "INFO - 17:05:43: Dictionary is loaded\n",
            "INFO - 17:05:43: Parsed Dataset is loaded\n",
            "INFO - 17:05:43: data_x is loaded\n",
            "INFO - 17:05:43: data_y is loaded\n",
            "INFO - 17:05:43: Tokenizer is loaded\n",
            "INFO - 17:05:43: Processed Data is Saved\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:35:25.694040Z",
          "start_time": "2019-08-16T18:35:25.571370Z"
        },
        "id": "U_WXX9HOf5no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y = dataset.get('train_x'), dataset.get('train_y')\n",
        "dev_x, dev_y = dataset.get('dev_x'), dataset.get('dev_y')\n",
        "vocabulary_size, output_size = dataset.get('vocabulary_size'), dataset.get('output_size')\n",
        "del dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4InAzRrKYJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras_self_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T17:33:09.079692Z",
          "start_time": "2019-08-16T17:33:09.069879Z"
        },
        "id": "axrrxo8Sf5nr",
        "colab_type": "code",
        "outputId": "b4c37616-7e76-43ad-bca0-5d028252418a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:13:37.435007Z",
          "start_time": "2019-08-16T18:13:37.427017Z"
        },
        "code_folding": [],
        "id": "RtbgTyaaf5nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model(vocabulary_size, hidden_size,\n",
        "                   embedding_size, output_size,\n",
        "                   lstm_layers=1, visualize=False, plot=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocabulary_size,\n",
        "                        output_dim=embedding_size, mask_zero=True))\n",
        "    for _ in range(lstm_layers):\n",
        "        model.add(Bidirectional(LSTM(hidden_size, dropout=0.2,\n",
        "                                     recurrent_dropout=0.2,\n",
        "                                     return_sequences=True,\n",
        "                                     input_shape=(None, None, embedding_size)),\n",
        "                                merge_mode='sum'))\n",
        "    model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    # Defining Adam optimizer\n",
        "    optimizer = Adam(lr=1e-6)\n",
        "    # Compiling the model\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    # To visualize the model\n",
        "    if visualize:\n",
        "        print('\\nModel Summary: \\n')\n",
        "        model.summary()\n",
        "    # Plot the model to have an image for it (report purposes)\n",
        "    if plot:\n",
        "        plot_model(model, to_file='BiLSTM Model.png')\n",
        "        logging.info(\"BiLSTM model image saved\")\n",
        "    logging.info('BiLSTM model is created & compiled')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT7ZDzXvKPsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_model(vocabulary_size, hidden_size,\n",
        "                    embedding_size, output_size,\n",
        "                    depth=2, visualize=False,\n",
        "                    plot=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocabulary_size,\n",
        "                        output_dim=embedding_size, mask_zero=True))\n",
        "    for _ in range(depth):\n",
        "        model.add(Bidirectional(LSTM(hidden_size, dropout=0.2,\n",
        "                                     recurrent_dropout=0.2,\n",
        "                                     return_sequences=True,\n",
        "                                     input_shape=(None, None, embedding_size)),\n",
        "                                merge_mode='sum'))\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "    model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    # Defining Adam optimizer\n",
        "    optimizer = Adam(lr=1e-6)\n",
        "    # Compiling the model\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    # To visualize the model\n",
        "    if visualize:\n",
        "        print('\\nModel Summary: \\n')\n",
        "        model.summary()\n",
        "    # Plot the model to have an image for it (report purposes)\n",
        "    if plot:\n",
        "        plot_model(model, to_file='Attention_BiLSTM_Model.png')\n",
        "        logging.info(\"Attention_BiLSTM model image saved\")\n",
        "    logging.info('Attention_BiLSTM model is created & compiled')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qm9lL5sTDaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "configure_tf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:55:02.079249Z",
          "start_time": "2019-08-16T18:55:02.075229Z"
        },
        "id": "tm9cLRiWf5nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1\n",
        "batch_size = 64\n",
        "hidden_size = 128\n",
        "embedding_size = 400"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:35:56.537305Z",
          "start_time": "2019-08-16T18:35:55.669655Z"
        },
        "id": "y5eRY9Prf5n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(type_flag, vocabulary_size, hidden_size,\n",
        "                 embedding_size, output_size, tokenizer=None,\n",
        "                 encode_decoder_data=None):\n",
        "    model = None\n",
        "    if type_flag == 'baseline':\n",
        "        model = baseline_model(vocabulary_size, hidden_size,\n",
        "                                embedding_size, output_size, tokenizer)\n",
        "    elif type_flag == 'attention':\n",
        "        model = attention_model(vocabulary_size, hidden_size,\n",
        "                                embedding_size, output_size)\n",
        "    elif type_flag == 'seq2seq' and encode_decoder_data is not None:\n",
        "        # TODO: To Implement\n",
        "        model = seq2seq_model(encode_decoder_data, vocabulary_size,\n",
        "                                hidden_size, embedding_size,\n",
        "                                visualize=True, plot=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH_ZoHJCJskc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type_flag = ['baseline', 'attention', 'seq2seq']\n",
        "model = create_model(type_flag[0], vocabulary_size, hidden_size,\n",
        "                     embedding_size, output_size, None)\n",
        "atten_model = create_model(type_flag[0], vocabulary_size, hidden_size,\n",
        "                           embedding_size, output_size, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07IpJ6PsWsud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Visualize training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history, save_to=None):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
        "\n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label=f'Training loss ({history.history[l][-1]:.5f})')\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label=f'Validation loss ({history.history[l][-1]:.5f})')\n",
        "    \n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    if save_to is not None:\n",
        "        plt.savefig(f'{save_to}_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label=f'Training accuracy ({history.history[l][-1]:.5f})')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label=f'Validation accuracy ({history.history[l][-1]:.5f})')\n",
        "\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    if save_to is not None:\n",
        "        plt.savefig(f'{save_to}_acc.png')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T17:33:27.286429Z",
          "start_time": "2019-08-16T17:33:27.280640Z"
        },
        "id": "_yVBOvvTf5n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = TensorBoard(\"logging/\")  # Log the model training process"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:54:26.229526Z",
          "start_time": "2019-08-16T18:54:26.222544Z"
        },
        "code_folding": [
          0
        ],
        "id": "EFPJw5Rgf5n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(data_x, data_y, batch_size, output_size):\n",
        "    for start in range(0, len(data_x), batch_size):\n",
        "        end = start + batch_size\n",
        "        data_x_, data_y_ = data_x[start:end], data_y[start:end]\n",
        "        max_len = len(max(data_x_, key=len))\n",
        "        data_x_ = pad_sequences(np.array(data_x_), padding='post',\n",
        "                                 maxlen=max_len)\n",
        "        data_y_ = pad_sequences(np.array(data_y_), padding='post',\n",
        "                                 maxlen=max_len)\n",
        "        # categorize outputs\n",
        "        data_y_ = to_categorical(data_y_, num_classes=output_size)\n",
        "\n",
        "        yield data_x_, data_y_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-16T18:55:14.979248Z",
          "start_time": "2019-08-16T18:55:11.644955Z"
        },
        "id": "5Hjs-FKLf5n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "ff880614-9c13-4738-9da9-8452baa24b87"
      },
      "source": [
        "batch_size = 16\n",
        "try:\n",
        "    history = model.fit_generator(data_generator(train_x, train_y, batch_size, output_size),\n",
        "                            verbose=1, shuffle=True, epochs=epochs,\n",
        "                            workers=0, use_multiprocessing=True,\n",
        "                            steps_per_epoch=len(train_x)//batch_size,\n",
        "                            callbacks=[logger])\n",
        "    history_path = os.path.join(os.getcwd(), 'resources', 'history.pkl')\n",
        "    save_pickle(history_path, history.history)\n",
        "    plot_history(history, os.path.join(os.getcwd(), 'resources', 'history'))\n",
        "    model.save_weights(os.path.join(os.getcwd(), 'resources', 'model_weights.h5'))\n",
        "except KeyboardInterrupt:\n",
        "    model.save_weights('model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING - 17:05:58: From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1585/2323 [===================>..........] - ETA: 4:30 - loss: 10.9803 - acc: 0.0745"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyKB53h3EL5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "try:\n",
        "    history = atten_model.fit_generator(data_generator(train_x, train_y, batch_size, output_size),\n",
        "                                        verbose=1, shuffle=True, epochs=epochs,\n",
        "                                        workers=0, use_multiprocessing=True,\n",
        "                                        steps_per_epoch=len(train_x)//batch_size,\n",
        "                                        callbacks=[logger])\n",
        "    history_path = os.path.join(os.getcwd(), 'resources', 'atten_history.pkl')\n",
        "    save_pickle(history_path, history.history)\n",
        "    plot_history(history, os.path.join(os.getcwd(), 'resources', 'atten_history'))\n",
        "    model.save_weights(os.path.join(os.getcwd(), 'resources', 'atten_model_weights.h5'))\n",
        "except KeyboardInterrupt:\n",
        "    model.save_weights('atten_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQeXvOHKDdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}